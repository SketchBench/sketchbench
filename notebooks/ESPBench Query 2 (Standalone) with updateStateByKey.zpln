{
  "paragraphs": [
    {
      "title": "Quantiles with UpdateStateByKey",
      "text": "%spark\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StringType, StructField, StructType, DoubleType, LongType, ArrayType, IntegerType}\nimport org.apache.kafka.clients.consumer.ConsumerConfig\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.util.LongAccumulator\nimport org.apache.spark.streaming._\nimport org.apache.spark.ml.feature.QuantileDiscretizer\nimport scala.util.parsing.json.JSON\nimport java.util.{Calendar, Date}\nimport java.text.SimpleDateFormat\nimport scala.collection.immutable.Map\n\nval ssc = new StreamingContext(sc, Seconds(2))\n\nval sqlContext = new SQLContext(sc)\nimport sqlContext.implicits._\n\nval r = scala.util.Random\nval groupId = s\"stream-nexmark-v${r.nextInt.toString}\"\n\nval batchesToRun = 60\n\nobject FinishedBatchesCounter {\n  @volatile private var instance: LongAccumulator = null\n\n  def getInstance(sc: SparkContext): LongAccumulator = {\n    if (instance == null) {\n      synchronized {\n        if (instance == null) {\n      \tinstance = sc.longAccumulator(\"FinishedBatchesCounter\")\n        }\n      }\n    }\n    instance\n  }\n}\n    \nval kafkaParams = Map[String, Object](\n  \"bootstrap.servers\" -> \"sketchbench-kafka:9092\",\n  \"key.deserializer\" -> classOf[StringDeserializer],\n  \"value.deserializer\" -> classOf[StringDeserializer],\n  \"group.id\" -> groupId,\n  \"auto.offset.reset\" -> \"latest\",\n  \"enable.auto.commit\" -> (true: java.lang.Boolean)\n)\n\nval topics = Array(\"SketchBench-1-1\")\nval stream = KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaParams)\n)\n\n\nval messages = stream\n    .map(record => record.value)\n    .flatMap(record => {\n         JSON.parseFull(record).map(rawMap =>{\n            val map = rawMap.asInstanceOf[Map[String,Map[String,Double]]]\n            val inMap = map.get(\"event\")\n            (\"bid\",inMap.get(\"bid\").toInt)\n         } )\n    })\n\nssc.checkpoint(\"hdfs://sketchbench-hdfs-namenodes:8020/zeppelin\") \n\ndef updateStateFunc(bids: Seq[Int], runningstate: Option[Array[(Double,Long,Long,Long,Long)]]):  Option[Array[(Double,Long,Long,Long,Long)]] = {\n    \n    val previousState = runningstate.getOrElse(Array((0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l)))\n    \n    val newState =Array((0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l),(0d,0l,0l,0l,0l))\n    \n    val totalRecords = bids.length\n    \n    val initArray = Array(0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)\n\n    val limitArray = initArray.map(_ * totalRecords)\n        \n    val sortedBids = bids.sortWith(_ < _)\n    print(\"Total Records =\"+totalRecords)\n    for(i <- 0 until initArray.length-1){\n        //println(\"=========\")\n        //println(i)\n        val next = i+1\n        val sliceStart = initArray(i) * totalRecords\n        val sliceEnd = initArray(next) * totalRecords\n        val subSeq = sortedBids.slice(sliceStart.toInt,sliceEnd.toInt)\n        //println(subSeq)\n        val mapValue =  previousState(i)\n        val min = mapValue._2\n        val max = mapValue._3\n        val count = mapValue._4\n        val total = mapValue._5\n\n        //println(count)\n        //println(total)\n        if(!subSeq.isEmpty){\n        val newCount = count + subSeq.size\n        val newTotal = count * total + subSeq.sum\n        val newMin =  if (min <= subSeq.min && min != 0 ) min else subSeq.min\n        val newMax =  if (max > subSeq.max) max else subSeq.max\n        //println(newCount)\n        //println(newTotal)\n        \n        newState(i) = (initArray(i),newMin,newMax,newCount,newTotal)\n        \n        }\n   \n    }\n\n    Some(newState)\n}\n\nval result = messages.updateStateByKey(updateStateFunc _)\n\nval result_map = result.flatMap(m => m._2)\n\nresult_map.print()\n\nssc.start()  \nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2021-07-31T06:46:00+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[33mwarning: \u001b[0mthere was one deprecation warning (since 1.0.6)\n\u001b[33mwarning: \u001b[0mthere was one deprecation warning (since 2.0.0)\n\u001b[33mwarning: \u001b[0mthere were two deprecation warnings in total; for details, enable `:setting -deprecation' or `:replay -deprecation'\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 4 times, most recent failure: Lost task 3.3 in stage 0.0 (TID 7, 10.4.10.11, executor 2): java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat $anonfun$messages$3(<console>:95)\n\tat scala.Option.map(Option.scala:230)\n\tat $anonfun$messages$2(<console>:92)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n  at scala.Option.foreach(Option.scala:407)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n  at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n  at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:736)\n  at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:735)\n  at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n  at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at scala.util.Try$.apply(Try.scala:213)\n  at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n  at java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.NoSuchElementException: None.get\n  at scala.None$.get(Option.scala:529)\n  at scala.None$.get(Option.scala:527)\n  at $anonfun$messages$3(<console>:95)\n  at scala.Option.map(Option.scala:230)\n  at $anonfun$messages$2(<console>:92)\n  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n  at org.apache.spark.scheduler.Task.run(Task.scala:127)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1627713082491_721813540",
      "id": "paragraph_1627146791237_1471000902",
      "dateCreated": "2021-07-31T06:31:22+0000",
      "dateStarted": "2021-07-31T06:46:00+0000",
      "dateFinished": "2021-07-31T06:46:15+0000",
      "status": "ERROR",
      "focus": true,
      "$$hashKey": "object:3930"
    },
    {
      "user": "anonymous",
      "dateUpdated": "2021-07-31T06:31:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1627713082492_286063485",
      "id": "paragraph_1627167362996_1677665305",
      "dateCreated": "2021-07-31T06:31:22+0000",
      "status": "READY",
      "$$hashKey": "object:3934"
    }
  ],
  "name": "ESPBench Query 2 (Standalone) with updateStateByKey",
  "id": "2GCHFPWX3",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/ESPBench Query 2 (Standalone) with updateStateByKey"
}